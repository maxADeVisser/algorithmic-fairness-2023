{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12394aa1-0fe2-49d1-951a-6defb73ef28c",
   "metadata": {},
   "source": [
    "# Algorithmic Fairness, Accountability, and Ethics, Spring 2023\n",
    "# Exercise 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccc72700",
   "metadata": {},
   "source": [
    "## Task 0 (Setup)\n",
    "\n",
    "We use the same dataset as in week 2 and 3. If you missed to install the module, please carry out the installation tasks at <https://github.com/zykls/folktables#basic-installation-instructions>.\n",
    "\n",
    "After successful installation, you should be able to run the following code to generate a prediction task.\n",
    "To make your life easier, we made the `BasicProblem`-magic from the `folktables` package (see exercises of week 2) explicit in this task.\n",
    "This way, you can get access to different encodings of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5921f706-cf73-4c13-a7da-0448ef057d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for 2018 1-Year person survey for CA...\n"
     ]
    }
   ],
   "source": [
    "from folktables.acs import adult_filter\n",
    "from folktables import ACSDataSource\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_source = ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "acs_data = data_source.get_data(states=[\"CA\"], download=True)\n",
    "\n",
    "feature_names = ['AGEP', # Age\n",
    "                 \"CIT\", # Citizenship status\n",
    "                 'COW', # Class of worker\n",
    "                 \"ENG\", # Ability to speak English\n",
    "                 'SCHL', # Educational attainment\n",
    "                 'MAR', # Marital status\n",
    "                 \"HINS1\", # Insurance through a current or former employer or union\n",
    "                 \"HINS2\", # Insurance purchased directly from an insurance company\n",
    "                 \"HINS4\", # Medicaid\n",
    "                 \"RAC1P\", # Recoded detailed race code\n",
    "                 'SEX']\n",
    "\n",
    "target_name = \"PINCP\" # Total person's income\n",
    "\n",
    "def data_processing(data, features, target_name:str, threshold: float = 35000):\n",
    "    df = data\n",
    "    ### Adult Filter (STARTS) (from Foltktables)\n",
    "    df = df[~df[\"SEX\"].isnull()]\n",
    "    df = df[~df[\"RAC1P\"].isnull()]\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PINCP'] > 100]\n",
    "    df = df[df['WKHP'] > 0]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    ### Adult Filter (ENDS)\n",
    "    ### Groups of interest\n",
    "    sex = df[\"SEX\"].values\n",
    "    ### Target\n",
    "    df[\"target\"] = df[target_name] > threshold\n",
    "    target = df[\"target\"].values\n",
    "    df = df[features + [\"target\", target_name]] ##we want to keep df before one_hot encoding to make Bias Analysis\n",
    "    df_processed = df[features].copy()\n",
    "    cols = [ \"HINS1\", \"HINS2\", \"HINS4\", \"CIT\", \"COW\", \"SCHL\", \"MAR\", \"SEX\", \"RAC1P\"]\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=False, columns=cols, drop_first=True)\n",
    "    df_processed = pd.get_dummies(df_processed, prefix=None, prefix_sep='_', dummy_na=True, columns=[\"ENG\"], drop_first=True)\n",
    "    return df_processed, df, target, sex\n",
    "\n",
    "data, data_original, target, group = data_processing(acs_data, feature_names, target_name)\n",
    "\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    data, target, group, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400bf53",
   "metadata": {},
   "source": [
    "# Task 1 (Decision tree)\n",
    "\n",
    "1. Train a decision tree classifier on the training dataset. (You can work on the original dataset or on the one-hot encoded one.) The following parameter choices worked well in our setup: `(DecisionTreeClassifier(min_samples_split = 0.01, min_samples_leaf= 0.01, max_features=\"auto\", max_depth = 15, criterion = \"gini\", random_state = 0))` Report on its accuracy. Visualize the tree using `plot_tree` from `sklearn`. Which parameters can you change to the adapt the size of the tree? Try to find parameters that make the tree easier to understand.\n",
    "2. For two training examples, explain their classification given the decision tree.\n",
    "3. Compute feature importance as shown in the lecture. Which features are most important?\n",
    "4. Compute permuted feature importance using sklearn as shown in the lecture. How does feature importance change?\n",
    "5. Provide a counterfactual for a feature vector that is predicted negatively. Compare to the counterfactual for logistic regression (last week's exercises). Is it a counterfactual in both models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b030b9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxvisser/.pyenv/versions/3.9.4/envs/algo_fairness/lib/python3.9/site-packages/sklearn/tree/_classes.py:269: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=15, max_features=&#x27;auto&#x27;, min_samples_leaf=0.01,\n",
       "                       min_samples_split=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=15, max_features=&#x27;auto&#x27;, min_samples_leaf=0.01,\n",
       "                       min_samples_split=0.01)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=15, max_features='auto', min_samples_leaf=0.01,\n",
       "                       min_samples_split=0.01)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(\n",
    "    min_samples_split=0.01,\n",
    "    min_samples_leaf=0.01,\n",
    "    max_features=\"auto\",\n",
    "    max_depth=15,\n",
    "    criterion=\"gini\",\n",
    ")\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc7dad",
   "metadata": {},
   "source": [
    "# Task 2 (Black-box model)\n",
    "\n",
    "1. Train a black-box model classifier (for example, use a random forest, a gradient-boosted decision tree, an SVM, or a Neural Network). Report on its accuracy. If you have used a tree data structure such as RF or gradient-boosted decision trees, report on the feature importance as in Task 1.\n",
    "2. Both for the decision tree and the black-box classifier, use the `shap` module to explain predictions. Contrast the two models to each other: What are similarities, how do they differ? As shown in the lecture, provide a summary plot, a dependence plot,  a  force plot for a negatively/positively predicted feature vector, and summary plot on the interaction values.\n",
    "3. Reflect on the explanations: How does the _decision tree_'s black-box explanation relate to its white-box explanation? Which classifier would you prefer when deploying a model as part of the machine learning pipeline? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algo_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "739e0ac42600ca0e0070443428662068beff961946874faded4845d31c096a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
